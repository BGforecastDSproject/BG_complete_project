{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf0a955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simennaess/Library/Python/3.8/lib/python/site-packages/statsmodels/compat/pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "df=pd.read_csv('/Users/simennaess/Documents/Master/Ukentlig_vareid_ys_kunde_time_lag_15_20.csv', parse_dates=['Ukedato'], sep=\";\", decimal=\",\", header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fc1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produkter hentet fra produktseleksjon (vasket og deretter hentet ut basert på at de står for 100%, 50% eller 10% \n",
    "#av maksvolumet for enkeltprodukter, samt sesongprodukter. Blant allProducts er det også 12 \n",
    "#ekstra produkter som ligger mellom mid-> low runners.)\n",
    "\n",
    "selectedTopProducts=[10133,9662,9399,9306,9400,10135,9898,9630,9897,9860]\n",
    "selectedMidProducts=[14759,9901,10019,14536,14508,9994,16444,16353,9995, 9307]\n",
    "selectedLowProducts=[14238,14288,14542,10002,10197,16408,16527, 14563, 9541] \n",
    "selectedSeasonalProducts=[10022, 9560, 10005, 9541]\n",
    "\n",
    "allSelectedProducts=[10133, 9662, 9399, 9306, 9400, 10135, 9898, 9630, 9897, 9860, 14759, \n",
    "                     9901, 10019, 14508, 9994, 16444, 16353, 9995, 9205, 14760, 15112, 14536, 16409, \n",
    "                     10022, 14540, 14251, 14490, 10017, 9560, 10005, 14238, 10002, 10197, 16408, 14288, 16527, 9541]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c476cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline and nodes\n",
    "from fedot.core.pipelines.pipeline import Pipeline\n",
    "from fedot.core.pipelines.node import PrimaryNode, SecondaryNode\n",
    "\n",
    "# Data \n",
    "from fedot.core.data.data import InputData\n",
    "from fedot.core.data.data_split import train_test_data_setup\n",
    "from fedot.core.repository.dataset_types import DataTypesEnum\n",
    "\n",
    "# Tasks\n",
    "from fedot.core.repository.tasks import Task, TaskTypesEnum, TsForecastingParams\n",
    "\n",
    "# Metric\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# Imports for creating plots\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 18, 7\n",
    "\n",
    "def encode_and_bind(original_dataframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe, columns=[feature_to_encode])\n",
    "    res = pd.concat([original_dataframe, dummies], axis=1).drop_duplicates()\n",
    "    \n",
    "    return(res)\n",
    "\n",
    "def product_iterator(df,products):\n",
    "    for p in products:\n",
    "        df1= df.groupby(['Ukedato','Vare_Id']).agg({'Sales':'sum'}).reset_index()\n",
    "        df2=df1.loc[df1['Vare_Id']==p]\n",
    "    \n",
    "    #One-hot encoder to get dummies for kjedeprofil\n",
    "\n",
    "    #df_dum_kjede=encode_and_bind(df2,'Kjedeprofil')\n",
    "    #df_dum_kjede.drop(['Kjedeprofil'],axis=1, inplace=True)\n",
    "    #df_dum_prod=encode_and_bind(df2,'Vare_Id')\n",
    "    #df_dum_prod.drop(['Vare_Id'],axis=1,inplace=True)\n",
    "\n",
    "    #df_dum=df_dum_prod.copy()\n",
    "    #df2=df_dum\n",
    "    return df2\n",
    "\n",
    "\n",
    "def fedot_mlp_hyperparamater_opti(product, horizon):            \n",
    "    print(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")\n",
    "    print(\"Vare_Id: \", product)\n",
    "    print(\"horizon: \", horizon)\n",
    "    \n",
    "    df2=df_grouped.loc[df_grouped['Vare_Id']==product]\n",
    "    df1= df2.groupby(['Ukedato','Vare_Id']).agg({'Sales':'sum'}).reset_index()\n",
    "\n",
    "    #'Price':'mean','Sales Promotion':'mean','Inventory':'sum','Vacation'\n",
    "    # :'mean','Christmas':'mean','Easter':'mean','Sales_lag_1':'sum','Sales_lag_2':\n",
    "\n",
    "    df1['Ukedato']=pd.to_datetime(df1['Ukedato'])                                                                                   # 'sum','Sales_lag_3':'sum'}).reset_index()\n",
    "\n",
    "    \n",
    "    # Plot data\n",
    "    #df1.plot('Ukedato', 'Sales')\n",
    "    #plt.title(product)\n",
    "    #plt.show()\n",
    "\n",
    "    #print(f'Length of the time series - {len(df1)}')\n",
    "\n",
    "    # numpy array with time series\n",
    "    sales = np.array(df1['Sales'])\n",
    "    #traffic_features=np.array(df.loc[:, df.columns != ['Ukedato','Sales'])\n",
    "    #traffic_features= np.array(df1[['Price','Sales Promotion','Sales_lag_1','Sales_lag_2','Sales_lag_3']])\n",
    "\n",
    "    forecast_length = horizon\n",
    "\n",
    "    # Wrapp data into InputData\n",
    "    task = Task(TaskTypesEnum.ts_forecasting,\n",
    "            TsForecastingParams(forecast_length=forecast_length))\n",
    "\n",
    "    # Get time series from dataframe\n",
    "    sales = np.array(df1['Sales'])\n",
    "    input_data = InputData(idx=np.arange(0, len(sales)),\n",
    "                           features=sales,\n",
    "                           #features=traffic_features,\n",
    "                           target=sales,\n",
    "                           task=task,\n",
    "                           data_type=DataTypesEnum.ts)\n",
    "\n",
    "    # Split data into train and test\n",
    "    train_input, predict_input = train_test_data_setup(input_data)\n",
    "\n",
    "    # Initialise pipeline for making forecasting\n",
    "    pipeline = get_two_branches_pipeline(lagged_param_1=144, \n",
    "                                         lagged_param_2=12)\n",
    "\n",
    "    # Fit\n",
    "    pipeline.fit(train_input)\n",
    "\n",
    "    # Make forecast\n",
    "    output = pipeline.predict(predict_input)\n",
    "    forecast = np.ravel(np.array(output.predict))\n",
    "\n",
    "    #plot_results(actual_time_series = sales,\n",
    "    #     predicted_values = forecast, \n",
    "    #     len_train_data = len(sales)-forecast_length)\n",
    "\n",
    "    # Print MAE metric\n",
    "    \n",
    "    print(f' MAPE: {mean_absolute_percentage_error(predict_input.target, forecast):.3f}')\n",
    "    print(f' RMSE: {np.sqrt(mean_squared_error(predict_input.target, forecast)):.3f}')\n",
    "\n",
    "    from fedot.core.pipelines.tuning.unified import PipelineTuner\n",
    "\n",
    "    init_pipeline = get_two_branches_pipeline()\n",
    "\n",
    "    # Call PipelineTuner\n",
    "    pipeline_tuner = PipelineTuner(pipeline=init_pipeline, \n",
    "                                   task=task,\n",
    "                                   iterations=50)\n",
    "\n",
    "    tuned_pipeline = pipeline_tuner.tune_pipeline(input_data=train_input,\n",
    "                                                  loss_function=mean_absolute_error,\n",
    "                                                  loss_params=None)\n",
    "\n",
    "    tuned_output = tuned_pipeline.predict(predict_input)\n",
    "    tuned_forecast = np.ravel(np.array(tuned_output.predict))\n",
    "\n",
    "    #plot_results(actual_time_series = sales,\n",
    "    #             predicted_values = tuned_forecast, \n",
    "    #             len_train_data = len(sales)-forecast_length)\n",
    "\n",
    "    # Print MAE metric\n",
    "    \n",
    "    print(f' MAPE tuned: {mean_absolute_percentage_error(predict_input.target, tuned_forecast):.3f}')\n",
    "    print(f' RMSE tuned: {np.sqrt(mean_squared_error(predict_input.target, tuned_forecast)):.3f}')\n",
    "\n",
    "    ## Fine tune all nodes- apporach til trening:\n",
    "\n",
    "    init_pipeline = get_two_branches_pipeline()\n",
    "\n",
    "    # Start tuning all nodes in the pipeline \n",
    "    tuned_pipeline = init_pipeline.fine_tune_all_nodes(loss_function=mean_absolute_error,\n",
    "                                                       loss_params=None,\n",
    "                                                       input_data=train_input,\n",
    "                                                       iterations=50)\n",
    "\n",
    "    # Make prediction\n",
    "    tuned_output = tuned_pipeline.predict(predict_input)\n",
    "    tuned_forecast = np.ravel(np.array(tuned_output.predict))\n",
    "\n",
    "\n",
    "    #plot_results(actual_time_series = sales,\n",
    "    #             predicted_values = tuned_forecast, \n",
    "    #             len_train_data = len(sales)-forecast_length)\n",
    "\n",
    "    # Print MAE metric\n",
    "    print(f' MAPE tuned 2: {mean_absolute_percentage_error(predict_input.target, tuned_forecast):.3f}')\n",
    "    print(f' RMSE tuned 2: {np.sqrt(mean_squared_error(predict_input.target, tuned_forecast)):.3f}')\n",
    "    \n",
    "    return [np.sqrt(mean_squared_error(predict_input.target, tuned_forecast)) ,mean_absolute_percentage_error(predict_input.target, tuned_forecast)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60db673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_run_evaluate_per_product(df,products,horizons):\n",
    "    total_rmse=0\n",
    "    total_mape=0\n",
    "    for product in products:\n",
    "        for horizon in horizons:\n",
    "            error=fedot_mlp_hyperparamater_opti(product,horizon)\n",
    "            total_rmse+=error[0]\n",
    "            total_mape+=error[1]\n",
    "            print(\"total RMSE for horisonten \",total_rmse, \"mape \", total_mape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d8586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_lengths=[3,12]\n",
    "\n",
    "selectedTopProducts=[10133,9662,9399,9306,9400,10135,9898,9630,9897,9860]\n",
    "selectedMidProducts=[14759,9901,10019,14536,14508,9994,16444,16353,9995, 9307]\n",
    "selectedLowProducts=[14238,14288,14542,10002,10197,16408,16527, 14563, 9541] \n",
    "selectedSeasonalProducts=[10022, 9560, 10005, 9541]\n",
    "\n",
    "allSelectedProducts=[10133, 9662, 9399, 9306, 9400, 10135, 9898, 9630, 9897, 9860, 14759, \n",
    "                     9901, 10019, 14508, 9994, 16444, 16353, 9995, 9205, 14760, 15112, 14536, 16409, \n",
    "                     10022, 14540, 14251, 14490, 10017, 9560, 10005, 14238, 10002, 10197, 16408, 14288, 16527, 9541]\n",
    "\n",
    "\n",
    "build_train_run_evaluate_per_product(df_grouped,[10133],[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a07a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_lengths=[3,12]\n",
    "\n",
    "build_train_run_evaluate_per_product(df_grouped,allSelectedProducts,forecast_lengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
